start_epoch: 0
epochs: 300

# number of validation steps to execute at the beginning of the training
# num_sanity_val_steps: 0
trainer: "lr_sync_train"

# ckpt path
checkpoint: null # /hkfs/work/workspace/scratch/qv2382-madonna-ddp/madonna/models/baseine/vit_b_16-imagenet/epoch43.pth.tar #
checkpoint_out_root: null # /hkfs/work/workspace/scratch/qv2382-madonna-ddp/madonna/models
save_period: 5
resume_run: False
resume: False  # TODO: are both of these needed??

criterion:
  _target_: torch.nn.CrossEntropyLoss
  label_smoothing: 0.1

lr: 0.001
lr_schedule:
  _target_: null
  # torch.optim.lr_scheduler.StepLR # torch.optim.lr_scheduler.CosineAnnealingWarmRestarts # torch.optim.lr_scheduler.CosineAnnealingLR #
  sched: cosine  # cosine LR scheduler from timm
  epochs: 300  # number of epochs for the cycle to run for
  warmup_epochs: 10
  cooldown_epochs: 10
  min_lr: 0.00001  # min LR of cosine annealing pattern
  warmup_lr: 0.00001  # starting point of warmup
  warmup_prefix: True  # default
  lr_cycle_mul: 1.5  # default -> cycle length: int(math.floor(-self.t_initial * (self.cycle_mul ** cycles - 1) / (1 - self.cycle_mul)))
  lr_cycle_limit: 30
  lr_cycle_decay: 0.4  # NOTE: the implementation is wrong in the docs!! it should be that the decay_rate is lr_cycle_decay!!!
  lr_k_decay: 1.0  # default 1.0
  sched_on_updates: True

sync_batchnorm: False

mixup: False
mixup_args:
  mixup_alpha: 0.8  # 0.8
  cutmix_alpha: 0.
  cutmix_minmax: null
  prob: 1.0  # 1.0
  switch_prob: 0.5
  mode: batch
  num_classes: null

optimizer:
  # _target_: torch.optim.SGD
  # _partial_: True
  # momentum: 0.9
  # weight_decay: 1.0e-5
  # nesterov: True
  _target_: torch.optim.AdamW
  _partial_: True
  weight_decay: 0.01  # default 0.01
  beta1: 0.9
  beta2: 0.999
  # fused: True
max_grad_norm: -1.

init_opt_with_model: False
print_freq: 50

# Model parameters
use_ddp: False
init_method: random-sigma  # options: random, unified, random-sigma
fixing_method:
  _target_: madonna.models.SVDSyncModel
  _partial_: True
  # stability_frequency: 100
  sync_delay: 0
  sigma_cutoff_fraction: 0.2
  # init_fraction: 1.0
  fixed_full_rank_layers:
    - head  # last layer
    - patch_embed.proj  # first layer
  # reinit_shapes: True
  # stable_update_delay: 1
  step_on_forward: True
  full_rank_warmup: False
  # use_ddp: False
  # distributed_updates: False
  # network_layer_perc_step: 0.1
  fixed_inner_dim: True
  inner_dim_init_ratio: 0.5
  random_sigma: False
  mixing_method: null  #  rbf, exp, null
  mixing_options:
    scaling: 10
  sync_min_size_fraction: 0.05
  # sync_frequency: 1000

non_svd_freq: 1
first_sync_epoch: 5

sync:
  first_epoch: 10
  epoch_freq: 10
  cutoff_fraction: 0.9
  vecs: 10
  method: all  # topn, random, fib, all
  sort: True
